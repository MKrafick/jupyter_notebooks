{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ee25e0",
   "metadata": {},
   "source": [
    "## Notebook: General_Database_Check\n",
    "\n",
    "#### Abstract\n",
    "<font color=blue> The purpose of this notebook is to run a generic database health check. This does not look for specific archetecture pre-requisites per se. The notebook does try to gather data for general healh, best practices, and key performance indicators. Various SQL was taken from other Jupyter Notebooks written by Ember Crooks or other performance and tuning articles, blogs, and resources. Various links to these methods are at the end of the notebook in the \"Credit and References\" section. </font>\n",
    "\n",
    "#### To Use This Notebook\n",
    "<font color=blue> \n",
    "    \n",
    "- If the libraries have not been installed, you will need to manually excute the *Install Notebook Prerequisites* section and restart the kernel.\n",
    "    \n",
    "- At a minimum you will need to manually *Import the Notebook Prerequisites* to load the libraries needed (this needs to be done only once after the notebook is opened that specific time).\n",
    "    \n",
    "- A variable file with connectivity information is expected to be held in the parent directory.\n",
    "    \n",
    "- Manually run the *Establish Database Connection*, type the password, and hit enter. \n",
    "    \n",
    "- Once done you can select the next section and choose \"Cells\" from the menu bar and then \"Run all Below\".\n",
    "    \n",
    "- Note: The SQL/Code cells are hidden to make it easier to read the generated output. You can turn that off and on in the next section. I reccomend exposing the SQL untill the whole notebook runs and then I would toggle off the SQL.\n",
    "</font>\n",
    "\n",
    "#### Printing/Saving a Report\n",
    "<font color=blue> \n",
    "The native Jupyter Notebook PDF generator seems to not respect hidden code cells and some data formatting is pretty ... awful. It may be more beneficial to hide the code cells and use the print function of your browser. You should be able to adjust to lanscape mode and save as a PDF.\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac165c01",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef00c2e8",
   "metadata": {},
   "source": [
    "## Code Toggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caddeff",
   "metadata": {},
   "source": [
    "<font color=blue> \n",
    "The code cells containing Python/SQL for this Jupyter notebook is hidden by default to make the notebook easier to read. To see view the queries used in this notebook, you will need to toggle on/off the raw code. <br>\n",
    "    \n",
    "<i>It is suggested that you expand all cells on your first run, then scroll back up to this section and hide the code blocks. This will make the report easier to read.</i>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d83f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package prerequisites so we can have code toggle so early in the notebook.\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "## Hide code cells\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<i><b> Toggle on/off the raw code by clicking <a href=\"javascript:code_toggle()\">here</a>.</b></i>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd11f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0daeb",
   "metadata": {},
   "source": [
    "## Install and Import Notebook Package Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec174ea6",
   "metadata": {},
   "source": [
    "<font color=blue> \n",
    "If you have not installed the following libraries in Python already (command line or via a notebook),\n",
    "you should run the following code block. These packages are required so the notebook can work.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f268497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this notebook is running on a Windows based machine with a Db2 client, you will need to run\n",
    "# the following to allow you to run Db2 commands (i.e. db2exfmt) not basic SQL\n",
    "\n",
    "import sys,os,os.path\n",
    "os.environ['IBM_DB_HOME']='/Applications/dsdriver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31798e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load importlib to give yourself the ability to import details from Python to see if certian libraries exist.\n",
    "# !pip3 install importlib     #Import library\n",
    "import importlib\n",
    "\n",
    "# The following will check for ibm_db_sa as a litmus test. If it exists, we assume the other libraries are installed.\n",
    "# If it does not exist, install the libraries.\n",
    "\n",
    "spec = importlib.util.find_spec(\"ibm_db_sa\")\n",
    "if spec is None:\n",
    "    print(\"Installing prerequisites.\")\n",
    "    # Dependancy: ipython-sql -> sqlalchemy -> ibm_db_sa -> ibm_db\n",
    "    !pip3 install ipython-sql #SQL Magic (%sql) using SQLAlchemy connect strings\n",
    "    !pip3 install ibm_db      #Db2 API for Python\n",
    "    !pip3 install ibm_db_sa   #Db2 adaptor for SQLAlchemy\n",
    "    !pip3 install matplotlib  #Plotting library for visualizations\n",
    "    !pip3 install pandas      #Data analysis and manipulation\n",
    "    \n",
    "    # Unnofficial community extentions, creates a new tab in Jupyter Notebook\n",
    "    # Uncheck \"disable configuration for nbextensions without explicit compatibility\" within new tab\n",
    "    !pip3 install https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tarball/master\n",
    "    !jupyter contrib nbextension install --user\n",
    "    print()\n",
    "else:\n",
    "    print(\"Prerequisites already installed, moving to import them.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6171bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and load SQL Magic\n",
    "import ibm_db\n",
    "import ibm_db_sa\n",
    "import sqlalchemy\n",
    "%load_ext sql\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "#import nbextensions\n",
    "%matplotlib inline\n",
    "import getpass\n",
    "\n",
    "# Configure SQL Magic in a few nice ways\n",
    "%config SqlMagic.style = 'MSWORD_FRIENDLY'\n",
    "pd.set_option('display.max_rows', 4096)\n",
    "pd.set_option('display.max_columns', 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ca911",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- Formatting for markup tables within the notebook -->\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Formatting for data tables within the notebook -->\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    text-align:left !important;\n",
    "    border: 1px  black solid !important;\n",
    "  color: black !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69784ea9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adaf388",
   "metadata": {},
   "source": [
    "## Establish Target Database Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45fd61",
   "metadata": {},
   "source": [
    "<font color=blue> \n",
    "Much of this notebook is dependant on connectivity information stored in a <i>ember_variables.py</i> file. This file can be found in 1Password. The file should be placed in the root directory of this notebook (i.e. one level up).\n",
    "    \n",
    "To connect to a target database, you will need to expand the code block and provide connections details including a schema name that will be used by SQL later in the notebook.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93511548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filename for passwords\n",
    "filename = 'connection_variables.py'\n",
    "# source the file\n",
    "%run ./$filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20daab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to target Db2 database\n",
    "\n",
    "# Connection Details\n",
    "inst=project_inst['PROD']\n",
    "user=project_user['PROD']\n",
    "host=project_host['PROD']\n",
    "schema='SCHEMANAME'  # Not necessary for connection, but used later to focus target of SQL\n",
    "db=project_db['PROD']\n",
    "port=project_port['PROD']\n",
    "\n",
    "# Prompt for Password\n",
    "password = getpass.getpass('Enter password for '+user)\n",
    "import urllib\n",
    "password = urllib.parse.quote(password) #Use to handle special characters\n",
    "\n",
    "# Connection String\n",
    "%sql db2+ibm_db://$user:$password@$host:$port/$db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d6ee3f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1d334",
   "metadata": {},
   "source": [
    "## Server Level Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1171b",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "From this perspective we are trying to get an idea of the size and shape of the database server. We are answering questions like \"is this a Linux box\", \"how much processing power do we have\", etc.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ef4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_server << \n",
    "SELECT\n",
    "    HOST_NAME,\n",
    "    OS_NAME AS OS,\n",
    "    OS_FULL_VERSION AS OS_VERSION,\n",
    "    OS_ARCH_TYPE,\n",
    "    CPU_ONLINE AS CPU,\n",
    "    CPU_CORES_PER_SOCKET AS CORES_PER_SOCKET,\n",
    "    MEMORY_TOTAL / 1024 AS MEMORY_GB,\n",
    "    MEMORY_SWAP_TOTAL / 1024 AS SWAP_GB,\n",
    "    MEMORY_SWAP_FREE / 1024 AS SWAP_FREE_GB\n",
    "FROM TABLE(SYSPROC.ENV_GET_SYSTEM_RESOURCES()) AS T\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736833ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Server Details\"))\n",
    "display(Markdown(\"Tip: Is swap space at least the same size of configured memory? (This does not apply to a containerized database). Rule of thumb is swap is at least 1:1 match to RAM and can be 1.5-2x on highly active systems. If your swap space is consistantly highly utilized (i.e. SWAP_FREE_GB) is less than 10%, you may need to increase swap space.\"))\n",
    "db_server_df=db_server.DataFrame()\n",
    "db_server_df.columns=db_server_df.columns.str.upper()\n",
    "db_server_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c9bb3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8db12",
   "metadata": {},
   "source": [
    "## Db2 Version and Licensing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b442692",
   "metadata": {},
   "source": [
    "<font color=blue> \n",
    "In this section we are trying to deduce what level of Db2 we are running, if we are licensed properly, and if we are properly licensed for features we may be using.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda05ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_ver << \n",
    "SELECT \n",
    "    INST_NAME, \n",
    "    SERVICE_LEVEL, \n",
    "    BLD_LEVEL, \n",
    "    FIXPACK_NUM \n",
    "FROM SYSIBMADM.ENV_INST_INFO \n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14469926",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Db2 Database Version\"))\n",
    "display(Markdown(\"Tip: If you see the word SPECIAL in the build level (BLD_LEVEL), this is a special build or fixpack IBM provided to address a HIPER APAR or some specific bug you discovered that required an edit in Db2 code. If you plan on an upgrade, you may want to check with IBM to confirm you can jump from your special build to a normal fixpack version.\"))\n",
    "db_ver_df=db_ver.DataFrame()\n",
    "db_ver_df.columns=db_ver_df.columns.str.upper()\n",
    "db_ver_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607aa93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_license << \n",
    "SELECT\n",
    "\tINSTALLED_PROD AS PRODUCT,\n",
    "\tINSTALLED_PROD_FULLNAME AS PROD_FULLNAME,\n",
    "\tPROD_RELEASE AS VERSION,\n",
    "\tLICENSE_INSTALLED,\n",
    "\tCASE\n",
    "    WHEN LICENSE_TYPE IS NULL THEN 'NO LICENSE INSTALLED'\n",
    "    ELSE LICENSE_TYPE\n",
    "  END AS LICENSE_TYPE,\n",
    "\tCASE\n",
    "    WHEN LICENSE_INSTALLED='Y' THEN '<-- ACTIVE LICENSE'\n",
    "\t  ELSE NULL\n",
    "\tEND\tAS STATUS\n",
    "FROM SYSIBMADM.ENV_PROD_INFO\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db199efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Db2 Licensing\"))\n",
    "db_license_df=db_license.DataFrame()\n",
    "db_license_df.columns=db_license_df.columns.str.upper()\n",
    "db_license_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_feature <<\n",
    "SELECT\n",
    "\tFEATURE_NAME AS DB2_FEATURE,\n",
    "\tFEATURE_FULLNAME AS FEATURE_FULLNAME,\n",
    "\tLICENSE_INSTALLED,\n",
    "\tPRODUCT_NAME AS PRODUCT,\n",
    "\tFEATURE_USE_STATUS\n",
    "FROM SYSIBMADM.ENV_FEATURE_INFO\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Db2 Feature and License Check\"))\n",
    "display(Markdown(\"If there are no licensable features in use, this will return and empty table.\"))\n",
    "db_feature_df=db_feature.DataFrame()\n",
    "db_feature_df.columns=db_feature_df.columns.str.upper()\n",
    "db_feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786395f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc66faeb",
   "metadata": {},
   "source": [
    "## Instance Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6a917",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "In this section we are looking at how the overall database instance is configured. Whatever settings are in place here apply to all databases within the instance.<br>\n",
    "    \n",
    "We are looking for details like how long Db2 has been up and running and what best practices may or may not be in place.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a714e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql instance_uptime <<\n",
    "SELECT \n",
    "    VARCHAR_FORMAT(DB2START_TIME, 'YYYY-MM-DD HH24:MM:SS') AS INSTANCE_ACTIVATION\n",
    "    FROM TABLE(MON_GET_INSTANCE(-2))\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e6e03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Db2 Instance Start time\"))\n",
    "instance_uptime_df=instance_uptime.DataFrame()\n",
    "instance_uptime_df.columns=instance_uptime_df.columns.str.upper()\n",
    "instance_uptime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f745b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_env_settings <<\n",
    "SELECT \n",
    "  REG_VAR_NAME AS DB2SET_PARM, \n",
    "  REG_VAR_VALUE AS DB2SET_VALUE\n",
    "FROM TABLE(ENV_GET_REG_VARIABLES(-1, 1)) \n",
    "WHERE REG_VAR_NAME IN ('DB2COMM','DB2AUTH','DB2_USE_ALTERNATE_PAGE_CLEANING','DB2_PARALLEL_IO','DB2_RESTORE_GRANT_ADMIN_AUTHORITIES','AUTOSTART')\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Common Db2 Environment Settings To Review For Best Practice\"))\n",
    "db_env_settings_df=db_env_settings.DataFrame()\n",
    "db_env_settings_df.columns=db_env_settings_df.columns.str.upper()\n",
    "db_env_settings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b2005",
   "metadata": {},
   "source": [
    "| Parameter   | Note |\n",
    "| :----------- | :----------- |\n",
    "| DB2_PARALLEL_IO | Changes I/O parrallelism by tablespace. Often set to * . If you have high prefetch wait times, the application may be waiting on pages and this may need to be adjusted.|\n",
    "| DB2COMM | Setting communication protocols. This is almost always TCPIP. |\n",
    "| DB2_USE_ALTERNATE_PAGE_CLEANING | Tells Db2 to be more proactive with page cleaning. Usually set to ON in transaction processing databases. |\n",
    "| DB2AUTH | Change authentication behavior. When LDAP is enabled, this should be OSAUTHDB. | \n",
    "|DB2_RESTORE_GRANT_ADMIN_AUTHORITIES | Usually set to ON. Immediately grants instance ownser SECADM which allows database restore from a backup taken in another instance (i.e. another server). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20919dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql mon_settings <<\n",
    "SELECT \n",
    "  NAME AS MONITOR_VALUE,\n",
    "  VALUE AS IS_ON\n",
    "FROM SYSIBMADM.DBMCFG\n",
    "WHERE NAME IN ('dft_mon_bufpool','dft_mon_lock','dft_mon_sort','dft_mon_stmt','dft_mon_table','dft_mon_timestamp','dft_mon_uow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b984f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Are Database Metrics Being Collected (DBM CFG)?\"))\n",
    "display(Markdown(\"My personal best practice is for all DFT_MON values should be set to ON.\"))\n",
    "mon_settings_df=mon_settings.DataFrame()\n",
    "mon_settings_df.columns=mon_settings_df.columns.str.upper()\n",
    "mon_settings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc54c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql key_dbm_config <<\n",
    "SELECT \n",
    "  NAME AS DBM_VALUE,\n",
    "  VALUE AS CURRENT_SETTING,\n",
    "  VALUE_FLAGS AS FLAGS\n",
    "FROM SYSIBMADM.DBMCFG\n",
    "WHERE NAME IN ('svcename','diagpath','diaglevel','instance_memory','intra_parallel','keystore_type','keystore_location')\n",
    "ORDER BY NAME \n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d727c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Key Database Manager Settings (DBM CFG)\"))\n",
    "key_dbm_config_df=key_dbm_config.DataFrame()\n",
    "key_dbm_config_df.columns=key_dbm_config_df.columns.str.upper()\n",
    "key_dbm_config_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7a0ee",
   "metadata": {},
   "source": [
    "| Parameter   | Note |\n",
    "| :----------- | :----------- |\n",
    "| diaglevel | Level of diagnostic messages captured in Db2 error log. Usually set to 3. All errors, warnings, event messages, and administration notification messages are captured. Some key informational messages might be captured at this level as well. |\n",
    "| diagpath | Where the Db2 error log is held (db2diag.log) |\n",
    "| instance_memory | Maximum amount of memory at instance level. When set to AUTOMATIC this is calculated between 75-90% of system memory. <b>Warning:</b> <i>This should be set to a cap on a containerized database. Otherwise Db2 will use node memory to calculate it's setting. This may starve out other workloads.</i> | \n",
    "| intra_parallel | Affects intrapartition query paralellism. Very dangerous to have on a OLTP database without heavy testing. Should be set to NO. |\n",
    "| keystore_location | Location of encryption keys. |\n",
    "| keystore_type | Type of keystore used to store encryption keys.\n",
    "| svcename | Name of the port listed in /etc/services. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b86877",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a2189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "108ce236",
   "metadata": {},
   "source": [
    "## Database Configuration & General Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbbbe2",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "In this section we are looking at how a database is configured and how it generally behaves.<br>\n",
    "    \n",
    "We are looking for details like how long the database has been up and running and what best practices may or may not be in place. It also allows us to look at general expected behavior and efficiency at a very high level. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819adb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql database_uptime <<\n",
    "SELECT\n",
    "VARCHAR_FORMAT(DB_CONN_TIME, 'YYYY-MM-DD HH24:MM:SS') AS DATABASE_ACTIVATION\n",
    "    FROM TABLE(MON_GET_DATABASE(-2))\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9391cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Database Start time\"))\n",
    "display(Markdown(\"Note: Many notebook calculations use cumulative database metrics collected from database start time. You could be looking at 2 days or 2 years of information. Metrics will change on database restart, which will reset the cumulative data.\"))\n",
    "database_uptime_df=database_uptime.DataFrame()\n",
    "database_uptime_df.columns=database_uptime_df.columns.str.upper()\n",
    "database_uptime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c8aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_last_bkup <<\n",
    "SELECT VARCHAR_FORMAT(LAST_BACKUP, 'YYYY-MM-DD HH24:MM:SS') AS LAST_DATABASE_BACKUP\n",
    "    FROM TABLE(MON_GET_DATABASE(-2))\n",
    "WITH UR;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Last Database Backup\"))\n",
    "display(Markdown(\"Are database backups being routinely taken?\"))\n",
    "db_last_bkup_df=db_last_bkup.DataFrame()\n",
    "db_last_bkup_df.columns=db_last_bkup_df.columns.str.upper()\n",
    "db_last_bkup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ea323",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql colocated <<\n",
    "SELECT\n",
    " CASE\n",
    "  WHEN TYPE = 'DB_STORAGE_PATH' THEN 'AUTOMATIC STORAGE PATH'\n",
    "  WHEN TYPE = 'LOGPATH' THEN 'ACTIVE LOG DIRECTORY PATH'\n",
    "  WHEN TYPE = 'DBPATH' THEN 'DATABASE DIRECTORY PATH'\n",
    "  ELSE TYPE\n",
    " END AS TYPE,\n",
    " SUBSTR(PATH,1,60) AS PATH\n",
    " FROM SYSIBMADM.DBPATHS\n",
    " WHERE TYPE NOT IN ('LOCAL_DB_DIRECTORY')\n",
    "UNION ALL\n",
    " SELECT\n",
    "  CASE\n",
    "   WHEN NAME = 'logarchmeth1' THEN 'ARCHIVE LOG DIRECTORY PATH'\n",
    "   ELSE NAME\n",
    "  END AS TYPE,\n",
    "  REPLACE(VALUE,'DISK:','') AS PATH\n",
    " FROM SYSIBMADM.DBCFG\n",
    " WHERE NAME IN ('logarchmeth1') AND VALUE LIKE 'DISK:%'\n",
    "UNION ALL\n",
    "SELECT\n",
    "  CASE\n",
    "   WHEN NAME = 'diagpath' THEN 'DB2 ERROR LOG PATH'\n",
    "   ELSE NAME\n",
    "  END AS TYPE,\n",
    "  VALUE AS PATH\n",
    " FROM SYSIBMADM.DBMCFG\n",
    " WHERE NAME IN ('diagpath')\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add5310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Colocated Filesystem Check\"))\n",
    "display(Markdown(\"Typically backend filesystems are seperated for active logging, archive logging, backup, error log, and data. This is muddied in a container or cloud based installation, but this practice should be respected whenever possible. It prevents I/O contention and one process choking out the space requirements of another process.\"))\n",
    "db_colocated_df=colocated.DataFrame()\n",
    "db_colocated_df.columns=db_colocated_df.columns.str.upper()\n",
    "db_colocated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql key_db_cfg << \n",
    "SELECT \n",
    "  NAME AS DB_VALUE,\n",
    "  VALUE AS CURRENT_SETTING,\n",
    "  VALUE_FLAGS AS FLAGS\n",
    "FROM SYSIBMADM.DBCFG \n",
    "WHERE NAME IN ('database_memory','locktimeout','logarchmeth1','logfilsiz','logprimary','logsecond','numlogspan', 'num_iocleaners','num_ioservers','logarchcompr1','self_tuning_mem')\n",
    "ORDER BY NAME\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b76af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Key Database Configuration Settings (DB CFG)\"))\n",
    "display(Markdown(\"There are MANY more configuration settings, this selection narrows our focus to a few key areas.\"))\n",
    "key_db_cfg_df=key_db_cfg.DataFrame()\n",
    "key_db_cfg_df.columns=key_db_cfg_df.columns.str.upper()\n",
    "key_db_cfg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873e2f86",
   "metadata": {},
   "source": [
    "| Parameter   | Note |\n",
    "| :----------- | :----------- |\n",
    "| database_memory | Amount of memory allocated to the database (subset of instance memory). AUTOMATIC allows Db2 to make decision on size it thinks is best based on current architecture and design. |\n",
    "| locktimeout | Number of seconds an application will wait to obtain a lock. If exceeded, the SQL times out. Usually set to 60 seconds. |\n",
    "| logarchcompr1 | Specifies if archive logs are compressed (saving space in the filesystem). |\n",
    "| logarchmeth1 | Specifies Db2 should use archive logging for point in time recovery and specifies where the archive logs should be held. May be local on server or in a S3 bucket. |\n",
    "| logfilsiz | Size of active logs. Often dictated by architecture and then tuned by DBA. |\n",
    "| logprimary | Number of primary active logs used by Db2. Varies by installation and prerequisites. |\n",
    "| logsecond | Number of secondary active logs used by Db2 when under load. Varies by installation and prerequisites. |\n",
    "| num_iocleaners | Number of asyncronous page cleaners used by the database. Rule of thumb is a cleaner for each physical CPU. When set to AUTOMATIC, it is set to a cleaner per physical core. |\n",
    "| num_ioservers | Number of prefetchers that can be used in a database at any one time for prefecthing and asyncronous I/O. AUTOMATIC sets this to parallellism settings of the tablespaces. |\n",
    "| self_tuning_mem | Tells Db2 to dynamically manage available memory for consumers enabled for self tuning. Typically this is always set to ON. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql hadr_state <<\n",
    "SELECT \n",
    "\tHADR_CONNECT_STATUS,\t\t\t\t\t\t\t\t\n",
    "\tHADR_STATE,\n",
    "\tHADR_LOG_GAP\n",
    "from table (MON_GET_HADR(0))\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### HADR Connection State\"))\n",
    "display(Markdown(\"If HADR is configured, this detail will show us if it is running and communicating normally.\"))\n",
    "hadr_state_df=hadr_state.DataFrame()\n",
    "hadr_state_df.columns=hadr_state_df.columns.str.upper()\n",
    "hadr_state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql hadr_config <<\n",
    "SELECT\n",
    "\tNAME AS DB_VALUE,\n",
    "\tVALUE AS CURRENT_SETTING\n",
    "FROM SYSIBMADM.DBCFG\n",
    "WHERE NAME IN ('hadr_local_host','hadr_local_svc','hadr_remote_host','hadr_remote_svc','hadr_syncmode','hadr_timeout','hadr_peer_window','hadr_target_list','hadr_replay_delay','logindexbuild','blocknonlogged','log_ddl_stmts')\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### HADR Database Configuration Settings (DB CFG)\"))\n",
    "display(Markdown(\"These parameters give us an idea of how high availability and disaster recovery (HADR) is configured as well as the settings for a few best practices.\"))\n",
    "hadr_config_df=hadr_config.DataFrame()\n",
    "hadr_config_df.columns=hadr_config_df.columns.str.upper()\n",
    "hadr_config_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73286ab0",
   "metadata": {},
   "source": [
    "| Parameter   | Note |\n",
    "| :----------- | :----------- |\n",
    "| blocknonlogged | Will prevent non-logged operations which helps ensure transaction logging is not bypassed so data is replicated to standby as normal. |\n",
    "| hadr_local_host | Hostname of the primary database server.|\n",
    "| hadr_local_svc | Port number HADR is listening on for the primary server. |\n",
    "| hadr_remote_host | Hostname of the standby database server.|\n",
    "| hadr_remote_svc | Port number HADR is listening on for the standby server. |\n",
    "| hadr_syncmode | Determines how log writes on the primary server are synced with log writes on the standby server. |\n",
    "| hadr_timeout | Number of seconds HADR process waits before considering a communication attempt to have failed. Usually set between 60 and 120.|\n",
    "| hadr_peer_window | Number of seconds the primary-standby pair continues to behave in peer state if primary loses connection to standby. Usually set between 60 and 120. |\n",
    "| logindexbuild | Index creation/recreation is logged on primary so they can be reconstructed on standby. Should be set to ON. |\n",
    "| log_ddl_stmts | DDL operations are recorded in logs on primary to provide more detail to the standby. Should be set to YES. |\n",
    "| hadr_target_list | Failover priority list when a failover takes place. Required when you have more than one standby. General best practice if you only have a single standby. |\n",
    "| hadr_replay_delay | Number of seconds from transaction commit on primary ro transaction commit on standby. Sometimes used to delay HADR state in case time is needed to disconnect the two because a bad transaction was about to be applied. Should be set to 0 for no delay. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a526a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_arss <<\n",
    "SELECT \n",
    "\tROWS_RETURNED,\n",
    "\tSELECT_SQL_STMTS,\n",
    "\tCASE \n",
    "\t\tWHEN SELECT_SQL_STMTS > 0 \n",
    "\t\t\tTHEN DECIMAL(FLOAT(rows_returned)/FLOAT(SELECT_SQL_STMTS),10,2)\n",
    "\t\t\tELSE 'No Div. By zero'\n",
    "\tEND AS AVG_RESULT_SET_SIZE,\n",
    "\tCASE \n",
    "\t\tWHEN DECIMAL(FLOAT(rows_returned)/FLOAT(SELECT_SQL_STMTS),10,2) <= 10 \n",
    "\t\t\tTHEN 'Transaction Processing Like Workload' \n",
    "\t\t\tELSE 'Warehouse Like Workload'\n",
    "\tEND AS WORKLOAD_TYPE\n",
    "FROM TABLE(MON_GET_DATABASE(-2))\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Database Average Result Set Size (ARSS)\"))\n",
    "display(Markdown(\"Does our database workload match what we expect it to be? Do we expect a very fast transactional workload but show a result set size typical of a warehouse or vise versa?\"))\n",
    "db_arss_df=db_arss.DataFrame()\n",
    "db_arss_df.columns=db_arss_df.columns.str.upper()\n",
    "db_arss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d2b85c",
   "metadata": {},
   "source": [
    "| Avg Result Set Size | Typical Workload Type |\n",
    "| :----------- | :----------- |\n",
    "| Less than 10 | Transaction Processing Workload |\n",
    "| More than 10 | Mixed Workload or Data Warehouse Workload |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df53bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_ixref <<\n",
    "SELECT ROWS_READ, \n",
    " ROWS_RETURNED,\n",
    " ROWS_READ/ROWS_RETURNED AS IXREF \n",
    "FROM TABLE(MON_GET_WORKLOAD('',-2)) AS T \n",
    "WHERE WORKLOAD_NAME='SYSDEFAULTUSERWORKLOAD' \n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Database Index Read Efficiency (IXREF)\"))\n",
    "display(Markdown(\"On average, for the database, how many rows are scanned before Db2 finds the row it needs? Often shows effectiveness of indexes and how they are used.\"))\n",
    "db_ixref_df=db_ixref.DataFrame()\n",
    "db_ixref_df.columns=db_ixref_df.columns.str.upper()\n",
    "db_ixref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93900cd7",
   "metadata": {},
   "source": [
    "| Index Read Efficiency | Efficiency Level |\n",
    "| :----------- | :----------- |\n",
    "| Less than 10 | Ideal (Especially for Transaction Processing) |\n",
    "| 10 to 100| Potentially acceptable. Possibly a mixed workload. |\n",
    "| 100 to 1000 | Poor. There are SQL tuning opportunities here. |\n",
    "| Greater than 1000 | Bad. Many SQL and DB tuning opportunities. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc24b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_rollbackpercent <<\n",
    "SELECT TOTAL_APP_COMMITS AS APP_COMMITS, \n",
    "\t   TOTAL_APP_ROLLBACKS AS APP_ROLLBACKS,\n",
    "\t   TOTAL_APP_COMMITS + TOTAL_APP_ROLLBACKS AS TOTAL,\n",
    "\t   CASE\n",
    "            WHEN TOTAL_APP_COMMITS + TOTAL_APP_ROLLBACKS > 0 THEN\n",
    "                DECIMAL(FLOAT(TOTAL_APP_ROLLBACKS)/FLOAT(TOTAL_APP_COMMITS + TOTAL_APP_ROLLBACKS),10,2)*100\n",
    "            ELSE 0\n",
    "        END AS PCNT_ROLLBACK\n",
    "FROM TABLE(MON_GET_DATABASE(-2))\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Database Transaction Rollback Percentage\"))\n",
    "display(Markdown(\"What percentage of your total workload is wasted on rolling back work that was in-flight? Anything over 1% would be uncommon. Excessive rollbacks can lead to locking and contention in database.\"))\n",
    "db_rollbackpercent_df=db_rollbackpercent.DataFrame()\n",
    "db_rollbackpercent_df.columns=db_rollbackpercent_df.columns.str.upper()\n",
    "db_rollbackpercent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee09281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_rollback= %sql SELECT TOTAL_APP_COMMITS AS APP_COMMITS, \\\n",
    "\tTOTAL_APP_ROLLBACKS AS APP_ROLLBACKS \\\n",
    "FROM TABLE(MON_GET_DATABASE(-2)) \\\n",
    "WITH UR\n",
    "\n",
    "db_rollback_df=db_rollback.DataFrame()\n",
    "db_rollback_df.sum().plot(kind='pie', autopct='%1.0f%%', ylabel=\" \", title='Percent of Database Rollbacks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03712fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql bp_ratio <<\n",
    "-- LISTAGG CHR10 inerts a newline which can not be displayed as a dataframe\n",
    "-- As a result we can not uppercase the column headers and use only the display command\n",
    "\n",
    "SELECT\n",
    "\tSUBSTR(BPHR.BP_NAME,1,18) AS BP_NAME,\n",
    "\tBP_CUR_BUFFSZ\n",
    "\tPAGESIZE,\n",
    "\t((PAGESIZE*BP_CUR_BUFFSZ)/1024)/1024 AS SZ_MB,\n",
    "\tTOTAL_LOGICAL_READS,\n",
    "\tTOTAL_PHYSICAL_READS,\n",
    "\tDATA_HIT_RATIO_PERCENT,\n",
    "\t(SELECT LISTAGG(TBSPACE,CHR(10)) \n",
    "\t\tWITHIN GROUP (ORDER BY CREATE_TIME) \n",
    "\t\tFROM SYSCAT.TABLESPACES TS, SYSCAT.BUFFERPOOLS B \n",
    "\t\tWHERE TS.BUFFERPOOLID = B.BUFFERPOOLID AND B.BPNAME=MGBP.BP_NAME) AS TABLESPACES\n",
    "FROM SYSIBMADM.BP_HITRATIO BPHR JOIN TABLE(MON_GET_BUFFERPOOL(NULL,-2)) MGBP\n",
    "\tON MGBP.BP_NAME=BPHR.BP_NAME\n",
    "    JOIN SYSCAT.BUFFERPOOLS SBP ON SBP.BPNAME=MGBP.BP_NAME\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca85be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Bufferpool Ratios and Tablespace Mapping\"))\n",
    "display(Markdown(\"The focus here is the Bufferpool Hit Ratio. Ideally you want to be 99-98%. Lower than 90 and there is probably opportunity for tuning.\"))\n",
    "display(bp_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql bp_tbl_cnt <<\n",
    "SELECT \n",
    "\tB.BPNAME AS BUFFERPOOL,\n",
    "\tB.BUFFERPOOLID AS BP_ID,\n",
    "\t(SELECT COUNT(*) \n",
    "\t\tFROM SYSCAT.TABLESPACES TS \n",
    "\t\tWHERE B.BUFFERPOOLID=TS.BUFFERPOOLID) AS TBLSPC_COUNT,\n",
    "\t(SELECT COUNT(*)\n",
    "       FROM SYSCAT.TABLES T JOIN SYSCAT.TABLESPACES TS\n",
    "\t   ON T.TBSPACE=TS.TBSPACE OR T.INDEX_TBSPACE=TS.TBSPACE OR T.LONG_TBSPACE=TS.TBSPACE\n",
    "       WHERE TS.BUFFERPOOLID=B.BUFFERPOOLID) AS TABLE_COUNT\n",
    "    FROM SYSCAT.BUFFERPOOLS B\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Mapping: Bufferpool to Tablespace and Table Count\"))\n",
    "display(Markdown(\"Often tablespaces and tables are grouped into asyncronous or synronus workloads and broken up into thier own tablespace or bufferpool. Critical or large tables may also be split off. This data shows general distrobution to give perspective of skew and mapping.\"))\n",
    "bp_tbl_cnt_df=bp_tbl_cnt.DataFrame()\n",
    "bp_tbl_cnt_df.columns=bp_tbl_cnt_df.columns.str.upper()\n",
    "bp_tbl_cnt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c2bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_invalid <<\n",
    "SELECT\n",
    "\tCASE OBJECTTYPE\n",
    "\t WHEN 'B' THEN 'TRIGGER'\n",
    "\t WHEN 'F' THEN 'ROUTINE'\n",
    "\t WHEN 'R' THEN 'USER-DEFINED DATA TYPE'\n",
    "\t WHEN 'V' THEN 'VIEW'\n",
    "\t WHEN 'v' THEN 'GLOBAL VARIABLE'\n",
    "\t WHEN 'y' THEN 'ROW PERMISSION'\n",
    "\t WHEN '2' THEN 'COLUMN MASK'\n",
    "\t WHEN '3' THEN 'USAGE LIST'\n",
    "\tEND\n",
    "\tOBJECT_TYPE,\n",
    "\tOBJECTSCHEMA AS\tSCHEMA ,\n",
    "\tROUTINENAME AS ROUTINE_NAME,\n",
    "\tOBJECTNAME AS OBJECT_NAME,\n",
    "\tSQLCODE,\n",
    "\tDATE(INVALIDATE_TIME)\tAS INVALID_DATE,\n",
    "\tTIME(INVALIDATE_TIME)\tAS INVALID_TIME,\n",
    "\tDATE(LAST_REGEN_TIME)\tAS LAST_REGEN\n",
    "FROM SYSCAT.INVALIDOBJECTS\n",
    "WHERE OBJECTSCHEMA = :schema\n",
    "ORDER BY OBJECT_TYPE, OBJECTSCHEMA\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0168bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Invalid Database Objects\"))\n",
    "display(Markdown(\"Are there any database objects that are marked invalid and need to be rebuilt or revalidated?\"))\n",
    "db_invalid_df=db_invalid.DataFrame()\n",
    "db_invalid_df.columns=db_invalid_df.columns.str.upper()\n",
    "db_invalid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f292f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0706a8",
   "metadata": {},
   "source": [
    "## Table Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc91a3",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "In this section we are looking at how efficiently SQL runs through the database by analyzing table behavior.<br>\n",
    "\n",
    "We can analyze general performance by looking at hot tables, what happens when a table is accessed, and how efficient an index may be. If database efficiency metrics in the previous section were not what we wanted or expected, this is a more granular look at why.    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql table_byreadoverflow << \n",
    "WITH t AS (\n",
    " SELECT\n",
    "      TABSCHEMA AS SCHEMA,\n",
    "      TABNAME AS TABLE,\n",
    "      ROWS_READ,\n",
    "      OVERFLOW_ACCESSES,\n",
    "      CASE WHEN ROWS_READ > 0 \n",
    "        THEN DECIMAL(FLOAT(OVERFLOW_ACCESSES)/FLOAT(ROWS_READ),10,2)*100 \n",
    "        ELSE 0 \n",
    "      END AS R_OVRFLW_PCT\n",
    " FROM TABLE(MON_GET_TABLE('','',-2)) AS t WHERE TABSCHEMA =:schema AND OVERFLOW_ACCESSES > 0)\n",
    "SELECT * FROM t WHERE R_OVRFLW_PCT >= 3 ORDER BY R_OVRFLW_PCT DESC\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa83f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Tables > 3% Read Overflows\"))\n",
    "display(Markdown(\"A read overflow is the result of Db2 looking for data where it expects to be and finds a pointer redirecting it to a new location that actually has the data. This causes a double logical read (essentially twice the work normally needed). It's a good practice to run a Db2 REORG on tables with more than 3% Read Overflows. This updates Db2's knowledge of where the data actually is.\"))\n",
    "table_byreadoverflow_df=table_byreadoverflow.DataFrame()\n",
    "table_byreadoverflow_df.columns=table_byreadoverflow_df.columns.str.upper()\n",
    "table_byreadoverflow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql table_bywriteoverflow << \n",
    "WITH t AS (\n",
    " SELECT\n",
    "      TABSCHEMA AS SCHEMA,\n",
    "      TABNAME AS TABLE,\n",
    "      ROWS_UPDATED,\n",
    "      OVERFLOW_CREATES,\n",
    "      CASE WHEN ROWS_UPDATED > 0 \n",
    "        THEN DECIMAL(FLOAT(OVERFLOW_CREATES)/FLOAT(ROWS_UPDATED),10,2)*100 \n",
    "        ELSE 0 \n",
    "      END AS W_OVRFLW_PCT\n",
    " FROM TABLE(MON_GET_TABLE('','',-2)) AS t WHERE TABSCHEMA =:schema AND OVERFLOW_CREATES > 0)\n",
    "SELECT * FROM t WHERE W_OVRFLW_PCT >= 3 ORDER BY W_OVRFLW_PCT DESC\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Tables > 3% Write Overflows\"))\n",
    "display(Markdown(\"Similiar to the Read Overflow, but this occurs when writing data. An update operation to VARCHAR causes the field to be longer and not fit on the original planned data page. A pointer is left and the data is written on another page. It's a good practice to run a Db2 REORG on tables with more than 3% Write Overflows.\"))\n",
    "table_bywriteoverflow_df=table_bywriteoverflow.DataFrame()\n",
    "table_bywriteoverflow_df.columns=table_bywriteoverflow_df.columns.str.upper()\n",
    "table_bywriteoverflow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57eee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql table_top10_byinsert << \n",
    "SELECT * \n",
    "FROM (SELECT ROW_NUMBER() OVER(ORDER BY SUM(ROWS_INSERTED) DESC) AS RANK, \n",
    "       TRIM(TABSCHEMA) as SCHEMA, \n",
    "       TRIM(TABNAME) as TABLE, \n",
    "       SUM(ROWS_INSERTED) as ROWS_INSERTED\n",
    "      FROM TABLE(MON_GET_TABLE('','',-2)) AS TB\n",
    "      WHERE TABSCHEMA=:schema\n",
    "      GROUP BY TABSCHEMA, TABNAME) \n",
    "WHERE RANK<=10\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ad896",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Top 10 Tables by Rows Inserted\"))\n",
    "table_top10_byinsert_df=table_top10_byinsert.DataFrame()\n",
    "table_top10_byinsert_df.columns=table_top10_byinsert_df.columns.str.upper()\n",
    "table_top10_byinsert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql table_top10_byread << \n",
    "SELECT * \n",
    "FROM (SELECT ROW_NUMBER() OVER(ORDER BY SUM(ROWS_READ) DESC) AS RANK, \n",
    "       TRIM(TABSCHEMA) AS SCHEMA, \n",
    "       TRIM(TABNAME) AS TABLE, \n",
    "       SUM(ROWS_READ) AS ROWS_READ\n",
    "      FROM TABLE(MON_GET_TABLE('','',-2)) AS TB \n",
    "      WHERE TABSCHEMA=:schema\n",
    "      GROUP BY TABSCHEMA, TABNAME) \n",
    "WHERE RANK<=10\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f820774",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Top 10 Tables by Rows Read\"))\n",
    "table_top10_byread_df=table_top10_byread.DataFrame()\n",
    "table_top10_byread_df.columns=table_top10_byread_df.columns.str.upper()\n",
    "table_top10_byread_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa1e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql table_top10_byupdate <<\n",
    "SELECT * \n",
    "FROM (SELECT ROW_NUMBER() OVER(ORDER BY SUM(ROWS_UPDATED) DESC) AS RANK, \n",
    "       TRIM(TABSCHEMA) AS SCHEMA, \n",
    "       TRIM(TABNAME) AS TABNAME, \n",
    "       SUM(ROWS_UPDATED) AS ROWS_UPDATED\n",
    "      FROM TABLE(MON_GET_TABLE('','',-2)) AS TB \n",
    "      WHERE TABSCHEMA=:schema\n",
    "      GROUP BY TABSCHEMA, TABNAME) \n",
    "WHERE RANK<=10\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Top 10 Tables by Rows Updated\"))\n",
    "table_top10_byupdate_df=table_top10_byupdate.DataFrame()\n",
    "table_top10_byupdate_df.columns=table_top10_byupdate_df.columns.str.upper()\n",
    "table_top10_byupdate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4da361",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql table_top10_bydelete <<\n",
    "SELECT * \n",
    "FROM (SELECT ROW_NUMBER() OVER(ORDER BY SUM(ROWS_DELETED) DESC) AS RANK,\n",
    "       TRIM(TABSCHEMA) AS SCHEMA, \n",
    "       TRIM(TABNAME) AS NAME, \n",
    "       SUM(ROWS_DELETED) AS ROWS_DELETED\n",
    "      FROM TABLE(MON_GET_TABLE('','',-2)) AS TB\n",
    "      WHERE TABSCHEMA=:schema\n",
    "      GROUP BY TABSCHEMA, TABNAME) \n",
    "WHERE RANK<=10\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Top 10 Tables by Rows Deleted\"))\n",
    "table_top10_bydelete_df=table_top10_bydelete.DataFrame()\n",
    "table_top10_bydelete_df.columns=table_top10_bydelete_df.columns.str.upper()\n",
    "table_top10_bydelete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3542c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql table_top10_byscan <<\n",
    "SELECT * \n",
    "FROM (SELECT ROW_NUMBER() OVER(ORDER BY SUM(TABLE_SCANS) DESC) AS RANK, \n",
    "       TRIM(TABSCHEMA) AS SCHEMA, \n",
    "       TRIM(TABNAME) AS TABLE, \n",
    "       SUM(TABLE_SCANS) AS NUM_SCANS\n",
    "      FROM TABLE(MON_GET_TABLE('','',-2)) AS TB \n",
    "      GROUP BY TABSCHEMA, TABNAME) \n",
    "WHERE RANK<=10\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Top 10 Tables by Number of Scans\"))\n",
    "table_top10_byscan_df=table_top10_byscan.DataFrame()\n",
    "table_top10_byscan_df.columns=table_top10_byscan_df.columns.str.upper()\n",
    "table_top10_byscan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql top_lowcard_indexes <<\n",
    "\n",
    "-- Find top 25 busiest tables on 100 rows or more\n",
    "-- Calculate index cardinality percentage compared to total table cardinality\n",
    "-- Show index cardinality percentage less than to equal to 3%\n",
    "-- Do not take (U)nique indexes into account because they are used to enforce uniqueness\n",
    "\n",
    "SELECT A.TABSCHEMA AS SCHEMA,\n",
    "       A.TABNAME AS TABLE,\n",
    "       A.INDNAME AS INDEX_NAME,\n",
    "       B.CARD AS TB_CARD,\n",
    "       A.FULLKEYCARD AS IX_FULLKEYCARD,\n",
    "       INT((FLOAT(A.FULLKEYCARD)/FLOAT(B.CARD)) * 100) AS PCNT_TBCARD\n",
    "FROM SYSCAT.INDEXES A INNER JOIN SYSCAT.TABLES B\n",
    "       ON A.TABSCHEMA = B.TABSCHEMA\n",
    "       AND A.TABNAME = B.TABNAME\n",
    "WHERE A.FULLKEYCARD > 0\n",
    "     AND A.TABSCHEMA <> 'SYSIBM'\n",
    " \t AND B.CARD > 100\n",
    " \t AND A.UNIQUERULE <> 'U'\n",
    " \t AND INT((FLOAT(A.FULLKEYCARD)/FLOAT(B.CARD)) * 100) <= 3\n",
    " \t AND A.TABSCHEMA=:schema\n",
    " \t AND A.TABNAME IN \n",
    "\t   (SELECT TABNAME\n",
    "       FROM TABLE(MON_GET_TABLE('','',-2)) \n",
    "       WHERE TABSCHEMA=:schema\n",
    "       ORDER BY ROWS_INSERTED + ROWS_UPDATED + ROWS_DELETED DESC\n",
    "       FETCH FIRST 25 ROWS ONLY)\n",
    "ORDER BY INT((FLOAT(A.FULLKEYCARD)/FLOAT(B.CARD)) * 100) ASC, B.CARD DESC\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Low Cardinality Indexes (of Top 25 Busiest Tables)\"))\n",
    "display(Markdown(\"The higher the FULLKEYCARD is (in relation to table cardinality) the more unique the value is. This leads to an efficient index. The lower FULLKEYCARD value means less unique values (1 being all values the same) which causes an inefficient index. This provides a starting point of what tables may need existing indexes tuned.\"))\n",
    "top_lowcard_indexes_df=top_lowcard_indexes.DataFrame()\n",
    "top_lowcard_indexes_df.columns=top_lowcard_indexes_df.columns.str.upper()\n",
    "top_lowcard_indexes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca5e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql top_tables_unused_indexes <<\n",
    "\n",
    "-- Make sure to exclude Primary Keys\n",
    "-- Watch out for Cluster Keys as MDC's don't seem to update LAST_USED\n",
    "\n",
    "SELECT\n",
    "\tTABSCHEMA,\n",
    "\tTABNAME,\n",
    "\tCOUNT(INDNAME) AS IDX_NOT_USED_CNT\n",
    "FROM SYSCAT.INDEXES\n",
    "WHERE LASTUSED='0001-01-01' AND tabschema=:schema AND UNIQUERULE != 'P'\n",
    "AND TABNAME IN\n",
    " \t   (SELECT TABNAME\n",
    "       FROM TABLE(MON_GET_TABLE('','',-2))\n",
    "       WHERE TABSCHEMA=:schema\n",
    "       ORDER BY ROWS_INSERTED + ROWS_UPDATED + ROWS_DELETED DESC\n",
    "       FETCH FIRST 25 ROWS ONLY)\n",
    "GROUP BY TABSCHEMA,TABNAME\n",
    "ORDER BY IDX_NOT_USED_CNT DESC\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Number of Indexes That Have Never Been Used (of Top 25 Busiest Tables)\"))\n",
    "display(Markdown(\"The more indexes a table has, the more of a performance impact on INSERT operations. These tables have high write activity but have indexes that are never used and could be slowing down INSERT performance.\"))\n",
    "top_tables_unused_indexes_df=top_tables_unused_indexes.DataFrame()\n",
    "top_tables_unused_indexes_df.columns=top_tables_unused_indexes_df.columns.str.upper()\n",
    "top_tables_unused_indexes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14555cce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a6efc",
   "metadata": {},
   "source": [
    "## Index Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a5805",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "In this section we are looking at how efficiently SQL runs through the database by analyzing Index Read Efficiency (IXREF).<br>\n",
    "\n",
    "We looked at this from a database perspective in the Database Configuration & General Performance section. The detail below will show what inefficient SQL drove that number. These SQL are scanning many rows to get to the row it is seeking.    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql stmt_top10_byixref <<\n",
    "WITH SUM_TAB (SUM_RR) AS (\n",
    "        SELECT FLOAT(SUM(ROWS_READ))\n",
    "        FROM TABLE(MON_GET_PKG_CACHE_STMT ( 'D', NULL, NULL, -2)) AS T)\n",
    "SELECT\n",
    "        INSERT_TIMESTAMP AS FIRST_INSRT_INTO_CACHE,\n",
    "        SUBSTR(STMT_TEXT,1,50) AS STATEMENT,\n",
    "        ROWS_READ,\n",
    "        DECIMAL(100*(FLOAT(ROWS_READ)/SUM_TAB.SUM_RR),5,2) AS PCT_TOT_RR,\n",
    "        ROWS_RETURNED,\n",
    "        CASE\n",
    "            WHEN ROWS_RETURNED > 0 THEN\n",
    "                DECIMAL(FLOAT(ROWS_READ)/FLOAT(ROWS_RETURNED),10,2)\n",
    "            ELSE -1\n",
    "        END AS IXREF,\n",
    "        NUM_EXECUTIONS\n",
    "    FROM TABLE(MON_GET_PKG_CACHE_STMT ( 'D', NULL, NULL, -2)) AS T, SUM_TAB\n",
    "    ORDER BY ROWS_READ DESC FETCH FIRST 10 ROWS ONLY \n",
    "    WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae42c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Top 10 Inefficient Statements (IXREF)\"))\n",
    "display(Markdown(\"Index Read Efficiency shows how many rows are scanned to fetch the row DB2 needs. Ideally in a OLTP environment this should be <10. High IXREF SQL that executes frequently could be a performance problem. Keep in mind, the timestamp of first insert into package cache can change on rerun as things are flushed out, etc. The data you are seeing are details since that first insert into the cache.\"))\n",
    "stmt_top10_byixref_df=stmt_top10_byixref.DataFrame()\n",
    "stmt_top10_byixref_df.columns=stmt_top10_byixref_df.columns.str.upper()\n",
    "stmt_top10_byixref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41674d29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2f2ac3",
   "metadata": {},
   "source": [
    "## Database Wait Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508c0e0",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "In this section we are looking at where our database spends time waiting as it does work. Waiting isn't necessarily bad if it isn't excessive and is in a more efficient area (ie. reading memory) than a less efficient area (ie. spinning physical disk).\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d48b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_timespent << select * \n",
    "    from table(mon_get_database(-2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04582074",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=db_timespent.DataFrame()\n",
    "df.columns= df.columns.str.upper()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: 'df' is a DataFrame that has time series data from MON_GET_DATABASE.\n",
    "# Because the Wait Times are cumulative, I really only care about the most \n",
    "# recent entry in 'df'.\n",
    "#\n",
    "# Get the index of the last row, and build a dictionary with with all of the \n",
    "# wait time elements:\n",
    "i = 0\n",
    "ix = {'AGENT_WAIT_TIME': df['AGENT_WAIT_TIME'][i],\n",
    "      'WLM_QUEUE_TIME_TOTAL': df['WLM_QUEUE_TIME_TOTAL'][i],\n",
    "      'LOCK_WAIT_TIME':  df['LOCK_WAIT_TIME'][i], \n",
    "      'LOG_BUFFER_WAIT_TIME': df['LOG_BUFFER_WAIT_TIME'][i],\n",
    "      'LOG_DISK_WAIT_TIME': df['LOG_DISK_WAIT_TIME'][i],\n",
    "      'TCPIP_RECV_WAIT_TIME': df['TCPIP_RECV_WAIT_TIME'][i],\n",
    "      'TCPIP_SEND_WAIT_TIME': df['TCPIP_SEND_WAIT_TIME'][i],\n",
    "      'IPC_RECV_WAIT_TIME': df['IPC_RECV_WAIT_TIME'][i],\n",
    "      'IPC_SEND_WAIT_TIME': df['IPC_SEND_WAIT_TIME'][i],\n",
    "      'FCM_RECV_WAIT_TIME': df['FCM_RECV_WAIT_TIME'][i],\n",
    "      'FCM_SEND_WAIT_TIME': df['FCM_SEND_WAIT_TIME'][i],\n",
    "      'AUDIT_SUBSYSTEM_WAIT_TIME': df['AUDIT_SUBSYSTEM_WAIT_TIME'][i],\n",
    "      'AUDIT_FILE_WRITE_WAIT_TIME': df['AUDIT_FILE_WRITE_WAIT_TIME'][i],\n",
    "      'DIAGLOG_WRITE_WAIT_TIME': df['DIAGLOG_WRITE_WAIT_TIME'][i],\n",
    "      'POOL_READ_TIME': df['POOL_READ_TIME'][i],\n",
    "      'POOL_WRITE_TIME': df['POOL_WRITE_TIME'][i],\n",
    "      'DIRECT_READ_TIME': df['DIRECT_READ_TIME'][i],\n",
    "      'DIRECT_WRITE_TIME': df['DIRECT_WRITE_TIME'][i],\n",
    "      'EVMON_WAIT_TIME': df['EVMON_WAIT_TIME'][i],\n",
    "      'TOTAL_EXTENDED_LATCH_WAIT_TIME': df['TOTAL_EXTENDED_LATCH_WAIT_TIME'][i],\n",
    "      'PREFETCH_WAIT_TIME': df['PREFETCH_WAIT_TIME'][i],\n",
    "      'COMM_EXIT_WAIT_TIME': df['COMM_EXIT_WAIT_TIME'][i],\n",
    "      'IDA_SEND_WAIT_TIME': df['IDA_SEND_WAIT_TIME'][i],\n",
    "      'IDA_RECV_WAIT_TIME': df['IDA_RECV_WAIT_TIME'][i],\n",
    "      'CF_WAIT_TIME': df['CF_WAIT_TIME'][i],\n",
    "      'RECLAIM_WAIT_TIME': df['RECLAIM_WAIT_TIME'][i],\n",
    "      'SPACEMAPPAGE_RECLAIM_WAIT_TIME': df['SPACEMAPPAGE_RECLAIM_WAIT_TIME'][i]}\n",
    "\n",
    "# Convert the dictionary into a new DataFrame.\n",
    "myx=pd.DataFrame.from_dict(ix, orient='index')\n",
    "\n",
    "# Calcuate the percentage of overall wait for each value.\n",
    "total=myx[0].sum()\n",
    "myx['PCT'] = myx.apply(lambda x: x[0]/total, axis=1)\n",
    "\n",
    "# Filter out wait elements that have <= 0.5% wait time), to avoid having \n",
    "# 27 slices of pie, most of which are tiny slivers\n",
    "myx = myx[myx['PCT'] >= 0.005]\n",
    "\n",
    "# Set up the graph\n",
    "display(Markdown(\"#### Database Wait Time\"))\n",
    "display(Markdown(\"Where is your database spending most of it's time waiting when doing work?\"))\n",
    "xlabel=myx.index.tolist()\n",
    "yname=myx.columns.values[0]\n",
    "plt = myx.plot(kind='pie',y=yname,labels=xlabel,figsize=[5,5],autopct='%0.1f%%')\n",
    "plt.get_legend().remove()\n",
    "plt.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e29c74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c17b64",
   "metadata": {},
   "source": [
    "## Locking Analysis (Past 24 Hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eab57a",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "Locking is a normal behavior in a database. However, cerian types of locking phenomena be a early warning sign for contention problems. This section will take a look at locking behavior over a 24 hour period.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql lock_event_24 <<\n",
    "SELECT\n",
    "  SUBSTR(EVENT_TYPE,1,18) AS EVENT_TYPE,\n",
    "  COUNT(*)/2 AS COUNT\n",
    "FROM DBAMON.LOCK_EVENT\n",
    "WHERE EVENT_TIMESTAMP >= CURRENT TIMESTAMP - 24 hours\n",
    "GROUP BY EVENT_TYPE \n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e3fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"####  Locking Events - Past 24 hours\"))\n",
    "display(Markdown(\"Locking is a natural part of work. The question is if any contention,timeouts, or waiting are excessive. Personal rule of thumb is anything under 10 occurances (that are not clustered in a short timeframe) over a 24 hour period is perfectly fine.\"))\n",
    "lock_event_24_df=lock_event_24.DataFrame()\n",
    "lock_event_24_df.columns=lock_event_24_df.columns.str.upper()\n",
    "lock_event_24_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c2d73c",
   "metadata": {},
   "source": [
    "| Event Type| Count |\n",
    "| :----------- | :----------- |\n",
    "| Deadlock | Two applications each hold a lock on a object the other needs. Db2 will choose a winner and a loser. The loser will lose it's lock so the other can proceed. Usually sign of an application problem. |\n",
    "| Locktimeout | One transaction is waiting for the lock on an object to be released and aborts when a specific time limit is reached. |\n",
    "| Lockwait | When a transaction is waiting for a lock on an object to be released. May eventually lead to a LOCKTIMEOUT if that pre-determined wait time is exceeded. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ca40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql lock_event_bytable_24 <<\n",
    "SELECT\n",
    "  SUBSTR(LP.TABLE_SCHEMA,1,18) AS TABLE_SCHEMA,\n",
    "  SUBSTR(LP.TABLE_NAME,1,30) AS TABLE_NAME,\n",
    "  SUBSTR(LE.EVENT_TYPE,1,18) AS LOCK_EVENT,\n",
    "  COUNT(*)/2 AS COUNT\n",
    "FROM DBAMON.LOCK_PARTICIPANTS LP, DBAMON.LOCK_EVENT LE\n",
    "WHERE LP.XMLID=LE.XMLID \n",
    "\tAND EVENT_TIMESTAMP >= CURRENT TIMESTAMP - 24 HOURS\n",
    "\tAND NOT (LP.TABLE_SCHEMA IS NULL OR LP.TABLE_SCHEMA IS NULL)\n",
    "GROUP BY LP.TABLE_SCHEMA, LP.TABLE_NAME, LE.EVENT_TYPE\n",
    "ORDER BY LP.TABLE_SCHEMA, LP.TABLE_NAME, LE.EVENT_TYPE \n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b3581",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"####  Locking Events By Table - Past 24 hours\"))\n",
    "display(Markdown(\"Are any of these locking events routinely happening in just a few tables?\"))\n",
    "lock_event_bytable_24_df=lock_event_bytable_24.DataFrame()\n",
    "lock_event_bytable_24_df.columns=lock_event_bytable_24_df.columns.str.upper()\n",
    "lock_event_bytable_24_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c466162",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b3afb",
   "metadata": {},
   "source": [
    "## Security and Access "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005f1ad",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "The intent is for this section to mature over time and be more of a comprehensive security check. But for now, we are looking at what access PUBLIC has to the database and what ID's may have superuser access.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql public_auth <<\n",
    "SELECT DISTINCT GRANTEE, GRANTEETYPE, 'DATABASE CONNECT AUTHORITY DETECTED' AS ACCESS_LEVEL \n",
    "   FROM SYSCAT.DBAUTH \n",
    "   WHERE GRANTEE='PUBLIC' AND CONNECTAUTH='Y'\n",
    " UNION\n",
    "SELECT DISTINCT GRANTEE, GRANTEETYPE, 'TABLE LEVEL AUTHORITY DETECTED' AS ACCESS_LEVEL \n",
    "   FROM SYSCAT.TABAUTH WHERE GRANTEE='PUBLIC'\n",
    "   AND TABSCHEMA NOT LIKE 'SYS%'\n",
    "   AND (CONTROLAUTH='Y' OR ALTERAUTH='Y' OR DELETEAUTH='Y' \n",
    "   OR DELETEAUTH='Y' OR INDEXAUTH='Y' OR INSERTAUTH='Y' \n",
    "   OR REFAUTH='Y' OR SELECTAUTH='Y' OR UPDATEAUTH='Y')\n",
    " UNION\n",
    "SELECT DISTINCT GRANTEE, GRANTEETYPE, 'PACKAGE LEVEL AUTHORITY DETECTED' AS ACCESS_LEVEL FROM SYSCAT.PACKAGEAUTH WHERE GRANTEE='PUBLIC' AND CONTROLAUTH='Y'\n",
    " UNION\n",
    "SELECT DISTINCT GRANTEE, GRANTEETYPE, 'SCHEMA LEVEL AUTHORITY DETECTED' AS ACCESS_LEVEL FROM SYSCAT.SCHEMAAUTH WHERE GRANTEE='PUBLIC'\n",
    "   AND SCHEMANAME NOT LIKE 'SYS%'\n",
    "   AND (ALTERINAUTH='Y' OR CREATEINAUTH='Y' OR DROPINAUTH='Y' \n",
    "   OR SELECTINAUTH='Y' OR INSERTINAUTH='Y' OR UPDATEINAUTH='Y' \n",
    "   OR DELETEINAUTH='Y' OR EXECUTEINAUTH='Y' OR SCHEMAADMAUTH='Y' \n",
    "   OR ACCESSCTRLAUTH='Y' OR DATAACCESSAUTH='Y' OR LOADAUTH='Y')\n",
    "ORDER BY GRANTEE, GRANTEETYPE, ACCESS_LEVEL\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"####  PUBLIC Access & Authorities\"))\n",
    "display(Markdown(\"Usually you want to avoid PUBLIC access to anything in the database. Sometimes PUBLIC access is found and is OK (i.e. ability to bind). What we are looking for is the ability for PUBLIC to connect and read or update sensitive information.\"))\n",
    "public_auth_df=public_auth.DataFrame()\n",
    "public_auth_df.columns=public_auth_df.columns.str.upper()\n",
    "public_auth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c54dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql dbadmin_access <<\n",
    "SELECT GRANTEE AS ID, 'DBADMIN GROUP' AS ACCESS\n",
    "FROM SYSCAT.DBAUTH\n",
    "WHERE DBADMAUTH='Y'\n",
    "UNION ALL\n",
    "SELECT GRANTEE, 'DATA ACCESS AUTH' AS ACCESS\n",
    "FROM SYSCAT.DBAUTH\n",
    "WHERE DATAACCESSAUTH='Y'\n",
    "ORDER BY ACCESS DESC\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550788f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"####  ID's with DBADMIN Access\"))\n",
    "display(Markdown(\"Are there any ID's that have higher level access than they should? Can they, and should they, have administration or data manipulation privledges.\"))\n",
    "dbadmin_access_df=dbadmin_access.DataFrame()\n",
    "dbadmin_access_df.columns=dbadmin_access_df.columns.str.upper()\n",
    "dbadmin_access_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631498c",
   "metadata": {},
   "source": [
    "| GROUP OR ACCESS LEVEL | Level of Access |\n",
    "| :----------- | :----------- |\n",
    "| DBADMIN Group | Administrative level access with almost unrestricted access to Db2 Commands with the ability to manipulate data. |\n",
    "| Data Access Auth | Read/Write privledge on all data and tables. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b38a40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690ed79",
   "metadata": {},
   "source": [
    "## Error Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b404ca",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "Error logs can tell you so much but are not often spot checked. This section will give you a glimpse of a 3 day period. Db2 can be chatty and put less than critical messages in the error log, but often you can see patterns of behavior or even the unexpected problem if you look closely enough.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4674ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql error_72hours <<\n",
    "SELECT \n",
    " TIMESTAMP, \n",
    " DBNAME,\n",
    " APPL_ID, \n",
    " LEVEL, \n",
    " IMPACT, \n",
    " SUBSTR(MSG,1, 70) AS MSG \n",
    "FROM TABLE (PD_GET_DIAG_HIST( 'MAIN', 'D', '', NULL, NULL) ) AS T \n",
    "WHERE LEVEL IN ('C','S','E') \n",
    "      AND MSG IS NOT NULL \n",
    "      AND TIMESTAMP >= current timestamp - 72 HOURS\n",
    "ORDER BY TIMESTAMP DESC\n",
    "WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9afd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Error Log - Past 72 hours\"))\n",
    "display(Markdown(\"Data below is not exhaustive. This is showing CRITICAL, SEVERE, or ERROR message in the db2diag.log. \"))\n",
    "error_72hours_df=error_72hours.DataFrame()\n",
    "error_72hours_df.columns=error_72hours_df.columns.str.upper()\n",
    "error_72hours_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d3f79",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb656cbe",
   "metadata": {},
   "source": [
    "## Credit & References\n",
    "\n",
    "- Ragu on Tech: [Db2 Finding Top 10 Most Active Tables](https://www.raghu-on-tech.com/2020/02/29/db2-finding-top-10-most-active-tables/)\n",
    "- Xtivia: [Index Read Efficiency a KPI for Db2](https://www.virtual-dba.com/blog/index-read-efficiency-db2/)\n",
    "- Datageek: [Db2 Table Scans](https://datageek.blog/en/2012/11/28/db2-table-scans/)\n",
    "- DBI Software Blog: [Table Read I/O and Overflows](https://www.dbisoftware.com/blog/db2_performance.php?id=116)\n",
    "- Xtivia: [Index Read Efficiency - A KPI for Db2](https://www.virtual-dba.com/blog/index-read-efficiency-db2/)\n",
    "- Datageek: [Boost Your Performance  (IREF) Index Read Efficiency](https://datageek.blog/en/2014/03/17/boost-your-performance-iref-index-read-efficiency/)\n",
    "- Use the Index Luke: [More Indexes, Slower Insert](https://use-the-index-luke.com/sql/dml/insert)\n",
    "- Statsology: [How to Create a Pie Chart from Pandas Dataframe](https://www.statology.org/pandas-pie-chart/)\n",
    "- Mark Gillis, Triton Consulting: IDUG NA D03 Jupyter Notebook Presentation (SQL and Formatting ideas scattered throughout this Notebook.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
